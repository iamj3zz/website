---
layout: work
title: Visual Sound Fusion
work_id: visual-sound-fusion
abstract: "An audiovisual collaboration with Maya Chen, merging live performance with generative visuals to create immersive multisensory experiences."
categories: [collabs, installations, live-acts]
primary_category: collabs
image: /assets/img/dumbpic.jpg
order: 26

# Define metadata once here
metadata:
  year: "2024"
  date: "April - December 2024"
  location: "Singapore / Berlin"
  places: "ArtScience Museum Singapore, CTM Festival Berlin"
  performed_in: "Asia, Europe"
  role: "Sound artist, co-creator"
  technology: "Real-time audio synthesis, generative visuals, TouchDesigner, Max/MSP, machine learning"
  collaborators: "Visual artist Maya Chen (co-creator, visual systems)"
  commissioned_by: "ArtScience Museum"
  produced_by: "Audiovisual Lab Singapore"
  curated_by: "Honor Harger"
  credits: "Sound: J3ZZ. Visuals: Maya Chen. Software development: J3ZZ & Maya Chen. Production: Audiovisual Lab Singapore. Technical direction: Wei Zhang"
  partners: "CTM Festival, Goethe-Institut"
  supporters: "National Arts Council Singapore"
  interview: "Feature in CreativeApplications.Net"
  press_kit: "Download at visualsoundfusion.art"
  socials: "@visualsoundfusion @j3zz_music @mayachen_art"
  special_thanks: "To the teams at ArtScience Museum and CTM Festival"
  custom:
    - label: "Format"
      value: "Installation & live performance"
    - label: "Duration"
      value: "6-month installation + 3 live performances"
    - label: "Visitors"
      value: "Over 45,000 at ArtScience Museum"

sections:
  - type: metadata
  - type: split-hero-metadata
    content_type: "image"
    image: /assets/img/dumbpic.jpg
    caption: "Installation view at ArtScience Museum, Singapore"

  - type: text
    title: "About the Collaboration"
    content: |
      *Visual Sound Fusion* is an **audiovisual collaboration** with Maya Chen, merging live performance with generative visuals to create immersive multisensory experiences. The project exists in two forms: as a permanent installation at ArtScience Museum Singapore and as a live performance piece that has toured internationally.

      The work explores the deep connection between sound and vision, using machine learning algorithms that allow the audio and visual elements to influence each other in real-time. J3ZZ's sound compositions generate parameters that control Maya Chen's visual systems, while the visual patterns feed back into the audio engine, creating a closed loop of audiovisual generation.

      In the installation format, visitors can interact with the system through motion sensors, becoming part of the generative process. In live performance format, both artists manipulate their respective systems on stage, creating improvised audiovisual compositions that emerge from the interplay between sound and image. The result is a truly synesthetic experience where it becomes impossible to separate what you hear from what you see.

  - type: iframe
    embed_code: '<iframe width="560" height="315" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allowfullscreen></iframe>'
    responsive: true
    aspect_ratio: "16:9"
    caption: "Live performance at CTM Festival Berlin"

  - type: image-grid
    columns: 3
    images:
      - /assets/img/dumbpic.jpg
      - /assets/img/dumbpic.jpg
      - /assets/img/dumbpic.jpg
      - /assets/img/dumbpic.jpg
      - /assets/img/dumbpic.jpg
      - /assets/img/dumbpic.jpg
    captions:
      - "Installation at ArtScience Museum"
      - "Visitors interacting with the work"
      - "Generative visual patterns"
      - "Live performance setup"
      - "J3ZZ and Maya Chen collaborating"
      - "Audiovisual feedback systems"

  - type: spacer
    height: "40px"

  - type: quote
    text: "A stunning synthesis of sound and vision that pushes the boundaries of audiovisual art. Visual Sound Fusion demonstrates the future of multisensory experience."
    author: "Honor Harger, Director, ArtScience Museum"
---
